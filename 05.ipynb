{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b5f6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "# INFO-F-422 -  Statistical Foundations of Machine Learning \n",
    "\n",
    "### Gian Marco Paldino - [gian.marco.paldino@ulb.be](mailto:gian.marco.paldino@ulb.be)\n",
    "### Cédric Simar - [cedric.simar@ulb.be](mailto:cedric.simar@ulb.be)\n",
    "\n",
    "## TP 5 - Ensembles of models and feature selection\n",
    "\n",
    "#### April 2023\n",
    "\n",
    "#### Materials originally developed by *Yann-Aël Le Borgne, Fabrizio Carcillo and Gianluca Bontempi*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e60bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "## Overview\n",
    "\n",
    "Feature selection and ensembles of models are two techniques which can be used to improve the accuracy of predictions.\n",
    "\n",
    "Feature selection aims at reducing the dimensionality of the problem, and is useful when input variables contain redundant or irrelevant (noisy) information. Benefits are twofold: it decreases the training time by simplifying the problem, and it decreases the complexity of the predictive model. This in turn usually improves the prediction accuracy, since high-dimensionality makes predictive models more prone to overfitting, and estimates of parameters more variant.\n",
    "s\n",
    "There are three main approaches to feature selection:\n",
    "- **Filter methods:** \n",
    "These methods rely solely on the data and their intrinsic properties, without considering the impact of the selected features on the learning algorithm performance. For this reason, they are often used as preprocessing techniques.\n",
    "\n",
    "- **Wrapper methods:** \n",
    "These methods assess subsets of variables according to their usefulness to a given predictor. The feature selection is performed using an evaluation function that includes the predictive performance of the considered learning algorithm as a selection criterion.\n",
    "\n",
    "- **Embedded methods:** \n",
    "These methods are specific to given learning machines, and usually built-in in the learning procedure (e.g. random forest, regularization based techniques).\n",
    "\n",
    "Ensembles of models consist in building several predictive models using resampled subsets of the original training set. The method works particularly well for predictive models with high variance (for example, decision trees or neural networks). The average prediction of the resulting models usually strongly decreases the variance component of the error, and as a consequence improves the prediction accuracy.\n",
    "\n",
    "In this session, we will illustrate both techniques using the IMDB 5000 dataset, which contains 27 variables describing 5043 movies. The variables contain information about the director, actors, number of Facebook likes for each actor, duration, genre, language, country, etc... We will use them to predict the movie success (through the IMDB score). The dataset together with a description of the variables is at https://www.kaggle.com/deepmatrix/imdb-5000-movie-dataset.\n",
    "\n",
    "The dataset is on the github of the course, in `5_EnsemblesFeatureSelection/movie_metadata.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f9d4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "## Preliminaries\n",
    "\n",
    "### Supervised learning\n",
    "\n",
    "The process of supervised learning involves the presence of an entity (the learner, also called prediction model), whose goal is to learn the mapping between inputs and outputs in a given problem.\n",
    "\n",
    "A supervised learning problem can be formulated as follows:\n",
    "\n",
    "\\[\n",
    " y = m(\\mathbf{x})\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\(y\\) represents the output variable (also called target)\n",
    "- \\(\\mathbf{x}\\) represents the vector of inputs (also called features).\n",
    "- \\(m\\) is the (unknown) mapping between input and outputs.\n",
    "\n",
    "In the majority of the supervised learning problems, the mapping \\(m\\) between input and outputs is unknown and needs to be estimated on the basis of the available input/output observation pairs \\((\\mathbf{x}_i,y_i)\\).\n",
    "\n",
    "## Classification vs regression\n",
    "\n",
    "Both classification and regression are sub-fields of *supervised learning*. In the two cases, we have predictive variables \\(\\mathbf{x}\\) and a target variable \\(y\\). \n",
    "The main difference between the two type of problems is the type of the target variable:\n",
    "\n",
    "- In classification, \\(y\\) is a discrete variable; i.e \\(y \\in \\{C_1,\\cdots,C_k\\}\\)\n",
    "- In regression, \\(y\\) is a continuous variable; i.e \\(y \\in \\mathbb{R}\\)\n",
    "\n",
    "In this practical, unlike the previous ones, we will tackle our problem as a regression problem, with the IMDB score being the continuous target variable to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d67f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "## Data overview and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612553ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"movie_metadata.csv\")\n",
    "np.random.seed(2)\n",
    "data = data.sample(1000) # Random subset of 1000 movies\n",
    "\n",
    "print(data.shape)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a2d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5ef578",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "We see there is a mix of categorical and numerical variables, and some missing values. In order to simplify the analysis, let us remove the categorical variables, and replace the NA values with the mean values of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc88c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify factor (categorical) variables\n",
    "factor_cols = data.select_dtypes(include=['object']).columns\n",
    "factor_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817de31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove categorical variables\n",
    "data_preprocessed = data.drop(columns=factor_cols)\n",
    "\n",
    "# Replace NaN with mean\n",
    "data_preprocessed = data_preprocessed.apply(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "data_preprocessed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae0ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "### Input and output variables\n",
    "\n",
    "The output variable (Y) is the `imdb_score`, and all other variables (X) are considered as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd9a186",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data_preprocessed['imdb_score'].values\n",
    "X = data_preprocessed.drop(columns=['imdb_score'])\n",
    "\n",
    "N = X.shape[0]\n",
    "n = X.shape[1]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(Y, bins=20)\n",
    "plt.title(\"Distribution of imdb_score\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean of Y:\", np.mean(Y))\n",
    "print(\"Variance of Y:\", np.var(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643b981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "### 1) Modelling with linear and decision tree models\n",
    "\n",
    "#### Linear model\n",
    "\n",
    "* Create a linear model for predicting the IMDB score on the basis of the other variables, and compute its empirical mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc25b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "Y_hat = model.predict(X)\n",
    "\n",
    "empirical_error = np.mean((Y_hat - Y)**2)\n",
    "print(\"Empirical error=\", round(empirical_error,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b142cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "* Which input variables are statistically correlated with the output?\n",
    "\n",
    "In Python, we can check the coefficients of the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952a54da",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.Series(model.coef_, index=X.columns)\n",
    "coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef90571",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "* Compute the validation error with a 10-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1434da01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=3)\n",
    "CV_err_lm_single_model = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_tr, X_ts = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_tr, Y_ts = Y[train_index], Y[test_index]\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_tr, Y_tr)\n",
    "    Y_hat_ts = model.predict(X_ts)\n",
    "    CV_err_lm_single_model.append(np.mean((Y_hat_ts - Y_ts)**2))\n",
    "\n",
    "print(\"CV error=\", round(np.mean(CV_err_lm_single_model),4), \n",
    "      \"; std dev=\", round(np.std(CV_err_lm_single_model),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ccf178",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "#### Decision tree\n",
    "\n",
    "* Modify the previous code to compute the empirical error using a decision tree model. Use sklearn's `DecisionTreeRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e7e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model = DecisionTreeRegressor(random_state=3)\n",
    "model.fit(X, Y)\n",
    "Y_hat = model.predict(X)\n",
    "\n",
    "empirical_error = np.mean((Y_hat - Y)**2)\n",
    "print(\"Empirical error=\", round(empirical_error,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b20316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "* Plot the resulting tree is more complicated in Python due to size, but we can just visualize its structure or get the depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df528ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_depth(), model.get_n_leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513593bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "* What is the 10-fold cross-validation error using a decision tree model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d53bcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_err_rpart_single_model = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_tr, X_ts = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_tr, Y_ts = Y[train_index], Y[test_index]\n",
    "    \n",
    "    model = DecisionTreeRegressor(random_state=3)\n",
    "    model.fit(X_tr, Y_tr)\n",
    "    Y_hat_ts = model.predict(X_ts)\n",
    "    CV_err_rpart_single_model.append(np.mean((Y_hat_ts - Y_ts)**2))\n",
    "\n",
    "print(\"CV error=\", round(np.mean(CV_err_rpart_single_model),4), \n",
    "      \"; std dev=\", round(np.std(CV_err_rpart_single_model),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6191c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "## 2) Ensemble of models\n",
    "\n",
    "Let us now create an ensemble of R=20 linear models to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b55d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 20\n",
    "CV_err_lm_ensemble_model = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_tr, X_ts = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_tr, Y_ts = Y[train_index], Y[test_index]\n",
    "    \n",
    "    Y_hat_ts_ensemble = np.zeros((X_ts.shape[0], R))\n",
    "    for r in range(R):\n",
    "        # Resample the training indices\n",
    "        idx_tr_resample = np.random.choice(train_index, size=len(train_index), replace=True)\n",
    "        X_tr_res = X.iloc[idx_tr_resample]\n",
    "        Y_tr_res = Y[idx_tr_resample]\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(X_tr_res, Y_tr_res)\n",
    "        Y_hat_ts_ensemble[:, r] = model.predict(X_ts)\n",
    "    \n",
    "    Y_hat_ts = np.mean(Y_hat_ts_ensemble, axis=1)\n",
    "    CV_err_lm_ensemble_model.append(np.mean((Y_hat_ts - Y_ts)**2))\n",
    "\n",
    "print(\"CV error=\", round(np.mean(CV_err_lm_ensemble_model),4), \n",
    "      \"; std dev=\", round(np.std(CV_err_lm_ensemble_model),4))\n",
    "\n",
    "# Is the CV error lower?\n",
    "print(\"Is ensemble error lower than single model?\", \n",
    "      np.mean(CV_err_lm_ensemble_model) < np.mean(CV_err_lm_single_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ac5d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "* Use a decision tree as the base model. Is the CV error lower?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441967f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 20\n",
    "CV_err_rpart_ensemble_model = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_tr, X_ts = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_tr, Y_ts = Y[train_index], Y[test_index]\n",
    "    \n",
    "    Y_hat_ts_ensemble = np.zeros((X_ts.shape[0], R))\n",
    "    for r in range(R):\n",
    "        idx_tr_resample = np.random.choice(train_index, size=len(train_index), replace=True)\n",
    "        X_tr_res = X.iloc[idx_tr_resample]\n",
    "        Y_tr_res = Y[idx_tr_resample]\n",
    "        \n",
    "        model = DecisionTreeRegressor(random_state=r)\n",
    "        model.fit(X_tr_res, Y_tr_res)\n",
    "        Y_hat_ts_ensemble[:, r] = model.predict(X_ts)\n",
    "    \n",
    "    Y_hat_ts = np.mean(Y_hat_ts_ensemble, axis=1)\n",
    "    CV_err_rpart_ensemble_model.append(np.mean((Y_hat_ts - Y_ts)**2))\n",
    "\n",
    "print(\"CV error=\", round(np.mean(CV_err_rpart_ensemble_model),4), \n",
    "      \"; std dev=\", round(np.std(CV_err_rpart_ensemble_model),4))\n",
    "\n",
    "# Is ensemble error lower?\n",
    "print(\"Is ensemble error lower than single tree model?\", \n",
    "      np.mean(CV_err_rpart_ensemble_model) < np.mean(CV_err_rpart_single_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d3b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "## 3) Feature selection\n",
    "\n",
    "### Filter methods\n",
    "\n",
    "#### Correlation with the output\n",
    "\n",
    "The following code performs feature selection by keeping the most correlated variables with the output. Compare the results for linear models and decision trees. What are the smallest CV errors, and how many features were needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f63c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = np.abs(X.corrwith(pd.Series(Y)))\n",
    "ranking_corr_idx = correlations.sort_values(ascending=False).index\n",
    "\n",
    "CV_err = np.zeros((n,10))\n",
    "\n",
    "fold_id = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_tr, X_ts = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_tr, Y_ts = Y[train_index], Y[test_index]\n",
    "    \n",
    "    for nb_features in range(1, n+1):\n",
    "        selected_features = ranking_corr_idx[:nb_features]\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_tr[selected_features], Y_tr)\n",
    "        Y_hat_ts = model.predict(X_ts[selected_features])\n",
    "        CV_err[nb_features-1, fold_id] = np.mean((Y_hat_ts - Y_ts)**2)\n",
    "    fold_id += 1\n",
    "\n",
    "for i in range(n):\n",
    "    print(\"#Features:\", i+1, \"; CV error=\", round(np.mean(CV_err[i,:]),4),\n",
    "          \"; std dev=\", round(np.std(CV_err[i,:]),4))\n",
    "\n",
    "print(\"Correlation ranking:\")\n",
    "print(ranking_corr_idx.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb2caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "#### mRMR\n",
    "\n",
    "We will implement a simple mRMR feature selection. mRMR uses mutual information. We will approximate mutual information via the correlation-based formula provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770f86e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_info_corr(X, Y):\n",
    "    c = np.corrcoef(X, Y)[0,1]\n",
    "    # Avoid invalid value if correlation == 1 or == -1\n",
    "    if abs(c)==1:\n",
    "        c = 0.999999\n",
    "    return -0.5 * np.log(1 - c**2)\n",
    "\n",
    "def compute_mi_vector(X_tr, Y_tr):\n",
    "    mis = []\n",
    "    for col in X_tr.columns:\n",
    "        mi = mutual_info_corr(X_tr[col].values, Y_tr)\n",
    "        mis.append(mi)\n",
    "    return np.array(mis)\n",
    "\n",
    "CV_err = np.zeros((n,10))\n",
    "\n",
    "fold_id = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_tr, X_ts = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_tr, Y_ts = Y[train_index], Y[test_index]\n",
    "    \n",
    "    mutual_info_values = compute_mi_vector(X_tr, Y_tr)\n",
    "    selected = []\n",
    "    candidates = list(range(n))\n",
    "    \n",
    "    for j in range(n):\n",
    "        redundancy_score = np.zeros(len(candidates))\n",
    "        if len(selected)>0:\n",
    "            # Compute pairwise mi between selected and candidates\n",
    "            mi_sc = []\n",
    "            for cidx in candidates:\n",
    "                col_c = X_tr.iloc[:, cidx]\n",
    "                mis_c = []\n",
    "                for sidx in selected:\n",
    "                    col_s = X_tr.iloc[:, sidx]\n",
    "                    # Compute mutual info between col_s and col_c\n",
    "                    cc = np.corrcoef(col_s, col_c)[0,1]\n",
    "                    if abs(cc)==1:\n",
    "                        cc=0.999999\n",
    "                    mis_c.append(-0.5*np.log(1-cc**2))\n",
    "                redundancy_score[candidates.index(cidx)] = np.mean(mis_c)\n",
    "        mRMR_score = mutual_info_values[candidates] - redundancy_score\n",
    "        best_idx = candidates[np.argmax(mRMR_score)]\n",
    "        selected.append(best_idx)\n",
    "        candidates.remove(best_idx)\n",
    "    \n",
    "    # selected is the ranking\n",
    "    for nb_features in range(1, n+1):\n",
    "        features_to_use = [X.columns[i] for i in selected[:nb_features]]\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_tr[features_to_use], Y_tr)\n",
    "        Y_hat_ts = model.predict(X_ts[features_to_use])\n",
    "        CV_err[nb_features-1, fold_id] = np.mean((Y_hat_ts - Y_ts)**2)\n",
    "    fold_id += 1\n",
    "\n",
    "for i in range(n):\n",
    "    print(\"#Features:\", i+1, \"; CV error=\", round(np.mean(CV_err[i,:]),4),\n",
    "          \"; std dev=\", round(np.std(CV_err[i,:]),4))\n",
    "    \n",
    "print(\"Selected features ranking (mRMR):\")\n",
    "print([X.columns[i] for i in selected])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02abf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "#### PCA\n",
    "\n",
    "The following code performs features selection by first transforming the inputs using PCA, and then keeping the most relevant principal components in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d01949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "CV_err = np.zeros((n,10))\n",
    "fold_id = 0\n",
    "for train_index, test_index in kf.split(X_pca):\n",
    "    X_tr, X_ts = X_pca[train_index], X_pca[test_index]\n",
    "    Y_tr, Y_ts = Y[train_index], Y[test_index]\n",
    "    \n",
    "    for nb_components in range(1, n+1):\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_tr[:, :nb_components], Y_tr)\n",
    "        Y_hat_ts = model.predict(X_ts[:, :nb_components])\n",
    "        CV_err[nb_components-1, fold_id] = np.mean((Y_hat_ts - Y_ts)**2)\n",
    "    fold_id += 1\n",
    "\n",
    "for i in range(n):\n",
    "    print(\"#Features:\", i+1, \"; CV error=\", round(np.mean(CV_err[i,:]),4),\n",
    "          \"; std dev=\", round(np.std(CV_err[i,:]),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7051042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "### Wrapper method: Forward selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d662579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward selection\n",
    "selected = []\n",
    "for round_i in range(n):\n",
    "    candidates = list(set(range(n)) - set(selected))\n",
    "    CV_err_temp = []\n",
    "    for c in candidates:\n",
    "        features_to_include = selected + [c]\n",
    "        fold_errors = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_tr, X_ts = X.iloc[train_index, features_to_include], X.iloc[test_index, features_to_include]\n",
    "            Y_tr, Y_ts = Y[train_index], Y[test_index]\n",
    "            \n",
    "            model = LinearRegression()\n",
    "            model.fit(X_tr, Y_tr)\n",
    "            Y_hat_ts = model.predict(X_ts)\n",
    "            fold_errors.append(np.mean((Y_hat_ts - Y_ts)**2))\n",
    "        CV_err_temp.append(np.mean(fold_errors))\n",
    "    \n",
    "    best_candidate = candidates[np.argmin(CV_err_temp)]\n",
    "    selected.append(best_candidate)\n",
    "    print(\"Round\", round_i+1, \"; Selected feature:\", best_candidate,\n",
    "          \"; CV error=\", round(min(CV_err_temp),4), \n",
    "          \"; std dev=\", round(np.std(fold_errors),4))\n",
    "\n",
    "print(\"Selected features:\", [X.columns[i] for i in selected])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b23764",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "## Further preprocessing to add categorical variables\n",
    "\n",
    "Categorical variables usually need to be transformed with 'one-hot-encoding' in order to be processed by a learning algorithm.\n",
    "\n",
    "In the following, we add some categorical variables (color, language, country, content_rating) to the preprocessed dataset, and use them to improve prediction performance.\n",
    "\n",
    "We can do the one-hot encoding using pandas `get_dummies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41510451",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_vars = factor_cols\n",
    "data_factor = data[factor_vars]\n",
    "\n",
    "variables_to_keep = [\"color\",\"language\",\"country\",\"content_rating\"]\n",
    "\n",
    "data_factor_onehot = pd.get_dummies(data_factor[variables_to_keep], prefix=variables_to_keep, dummy_na=False)\n",
    "\n",
    "data_preprocessed_extended = pd.concat([data_preprocessed, data_factor_onehot], axis=1)\n",
    "X = data_preprocessed_extended.drop(columns=['imdb_score'])\n",
    "Y = data_preprocessed_extended['imdb_score'].values\n",
    "N, n = X.shape\n",
    "\n",
    "X.columns = [c.replace(\" \", \"_\").replace(\"-\", \"_\") for c in X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219acba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "## Using other predictive models\n",
    "\n",
    "We can try other models like SVM (SVR), Neural Networks (MLPRegressor), K-Nearest Neighbors (KNeighborsRegressor) and see if performance improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9ab0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "CV_err_svm_single_model = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_tr, X_ts = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_tr, Y_ts = Y[train_index], Y[test_index]\n",
    "    \n",
    "    model = SVR()\n",
    "    model.fit(X_tr, Y_tr)\n",
    "    Y_hat_ts = model.predict(X_ts)\n",
    "    CV_err_svm_single_model.append(np.mean((Y_hat_ts - Y_ts)**2))\n",
    "\n",
    "print(\"CV error=\",round(np.mean(CV_err_svm_single_model),4),\n",
    "      \"; std dev=\", round(np.std(CV_err_svm_single_model),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection with mRMR for SVM\n",
    "\n",
    "n_variables = 10\n",
    "\n",
    "CV_err_svm_single_model_fs = np.zeros((n_variables,10))\n",
    "fold_id = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_tr, X_ts = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_tr, Y_ts = Y[train_index], Y[test_index]\n",
    "    \n",
    "    mutual_info_values = compute_mi_vector(X_tr, Y_tr)\n",
    "    selected = []\n",
    "    candidates = list(range(n))\n",
    "    \n",
    "    for j in range(n_variables):\n",
    "        redundancy_score = np.zeros(len(candidates))\n",
    "        if len(selected)>0:\n",
    "            for ci, cidx in enumerate(candidates):\n",
    "                col_c = X_tr.iloc[:, cidx].values\n",
    "                mis_c = []\n",
    "                for sidx in selected:\n",
    "                    col_s = X_tr.iloc[:, sidx].values\n",
    "                    cc = np.corrcoef(col_s, col_c)[0,1]\n",
    "                    if abs(cc)==1:\n",
    "                        cc=0.999999\n",
    "                    mis_c.append(-0.5*np.log(1-cc**2))\n",
    "                redundancy_score[ci] = np.mean(mis_c)\n",
    "        mRMR_score = mutual_info_values[candidates] - redundancy_score\n",
    "        best_idx = candidates[np.argmax(mRMR_score)]\n",
    "        selected.append(best_idx)\n",
    "        candidates.remove(best_idx)\n",
    "        \n",
    "    # Evaluate performance with subsets of selected features\n",
    "    for nb_features in range(1, n_variables+1):\n",
    "        feats = [X.columns[i] for i in selected[:nb_features]]\n",
    "        model = SVR()\n",
    "        model.fit(X_tr[feats], Y_tr)\n",
    "        Y_hat_ts = model.predict(X_ts[feats])\n",
    "        CV_err_svm_single_model_fs[nb_features-1, fold_id] = np.mean((Y_hat_ts - Y_ts)**2)\n",
    "    fold_id+=1\n",
    "\n",
    "for i in range(n_variables):\n",
    "    print(\"#Features:\", i+1, \"; CV error=\",round(np.mean(CV_err_svm_single_model_fs[i,:]),4),\n",
    "          \"; std dev=\", round(np.std(CV_err_svm_single_model_fs[i,:]),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ea98ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "CV_err_nnet_single_model = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_tr, X_ts = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_tr, Y_ts = Y[train_index], Y[test_index]\n",
    "    \n",
    "    # Rescale output to 0-1\n",
    "    Y_tr_rescale = Y_tr/10.0\n",
    "    model = MLPRegressor(hidden_layer_sizes=(10,), max_iter=5000, random_state=3)\n",
    "    model.fit(X_tr, Y_tr_rescale)\n",
    "    Y_hat_ts = model.predict(X_ts)*10.0\n",
    "    CV_err_nnet_single_model.append(np.mean((Y_hat_ts - Y_ts)**2))\n",
    "\n",
    "print(\"CV error=\",round(np.mean(CV_err_nnet_single_model),4),\n",
    "      \"; std dev=\", round(np.std(CV_err_nnet_single_model),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c1e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection for MLP using mRMR\n",
    "CV_err_nnet_single_model_fs = np.zeros((n_variables,10))\n",
    "fold_id=0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_tr, X_ts = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_tr, Y_ts = Y[train_index], Y[test_index]\n",
    "    \n",
    "    mutual_info_values = compute_mi_vector(X_tr, Y_tr)\n",
    "    selected = []\n",
    "    candidates = list(range(n))\n",
    "    \n",
    "    for j in range(n_variables):\n",
    "        redundancy_score = np.zeros(len(candidates))\n",
    "        if len(selected)>0:\n",
    "            for ci, cidx in enumerate(candidates):\n",
    "                col_c = X_tr.iloc[:, cidx].values\n",
    "                mis_c = []\n",
    "                for sidx in selected:\n",
    "                    col_s = X_tr.iloc[:, sidx].values\n",
    "                    cc = np.corrcoef(col_s, col_c)[0,1]\n",
    "                    if abs(cc)==1:\n",
    "                        cc=0.999999\n",
    "                    mis_c.append(-0.5*np.log(1-cc**2))\n",
    "                redundancy_score[ci] = np.mean(mis_c)\n",
    "        mRMR_score = mutual_info_values[candidates] - redundancy_score\n",
    "        best_idx = candidates[np.argmax(mRMR_score)]\n",
    "        selected.append(best_idx)\n",
    "        candidates.remove(best_idx)\n",
    "        \n",
    "    for nb_features in range(1, n_variables+1):\n",
    "        feats = [X.columns[i] for i in selected[:nb_features]]\n",
    "        Y_tr_rescale = Y_tr/10.0\n",
    "        model = MLPRegressor(hidden_layer_sizes=(10,), max_iter=500, random_state=3)\n",
    "        model.fit(X_tr[feats], Y_tr_rescale)\n",
    "        Y_hat_ts = model.predict(X_ts[feats])*10.0\n",
    "        CV_err_nnet_single_model_fs[nb_features-1, fold_id] = np.mean((Y_hat_ts - Y_ts)**2)\n",
    "    fold_id+=1\n",
    "\n",
    "for i in range(n_variables):\n",
    "    print(\"#Features:\", i+1, \"; CV error=\",round(np.mean(CV_err_nnet_single_model_fs[i,:]),4),\n",
    "          \"; std dev=\", round(np.std(CV_err_nnet_single_model_fs[i,:]),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea8b8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "CV_err_lazy_single_model = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_tr, X_ts = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_tr, Y_ts = Y[train_index], Y[test_index]\n",
    "    \n",
    "    model = KNeighborsRegressor(n_neighbors=5)\n",
    "    model.fit(X_tr, Y_tr)\n",
    "    Y_hat_ts = model.predict(X_ts)\n",
    "    CV_err_lazy_single_model.append(np.mean((Y_hat_ts - Y_ts)**2))\n",
    "\n",
    "print(\"CV error=\",round(np.mean(CV_err_lazy_single_model),4),\n",
    "      \"; std dev=\", round(np.std(CV_err_lazy_single_model),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e4b07f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Feature selection for KNN using mRMR\n",
    "CV_err_lazy_single_model_fs = np.zeros((n_variables,10))\n",
    "fold_id=0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_tr, X_ts = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_tr, Y_ts = Y[train_index], Y[test_index]\n",
    "    \n",
    "    mutual_info_values = compute_mi_vector(X_tr, Y_tr)\n",
    "    selected = []\n",
    "    candidates = list(range(n))\n",
    "    \n",
    "    for j in range(n_variables):\n",
    "        redundancy_score = np.zeros(len(candidates))\n",
    "        if len(selected)>0:\n",
    "            for ci, cidx in enumerate(candidates):\n",
    "                col_c = X_tr.iloc[:, cidx].values\n",
    "                mis_c = []\n",
    "                for sidx in selected:\n",
    "                    col_s = X_tr.iloc[:, sidx].values\n",
    "                    cc = np.corrcoef(col_s, col_c)[0,1]\n",
    "                    if abs(cc)==1:\n",
    "                        cc=0.999999\n",
    "                    mis_c.append(-0.5*np.log(1-cc**2))\n",
    "                redundancy_score[ci] = np.mean(mis_c)\n",
    "        mRMR_score = mutual_info_values[candidates] - redundancy_score\n",
    "        best_idx = candidates[np.argmax(mRMR_score)]\n",
    "        selected.append(best_idx)\n",
    "        candidates.remove(best_idx)\n",
    "        \n",
    "    for nb_features in range(1, n_variables+1):\n",
    "        feats = [X.columns[i] for i in selected[:nb_features]]\n",
    "        model = KNeighborsRegressor(n_neighbors=5)\n",
    "        model.fit(X_tr[feats], Y_tr)\n",
    "        Y_hat_ts = model.predict(X_ts[feats])\n",
    "        CV_err_lazy_single_model_fs[nb_features-1, fold_id] = np.mean((Y_hat_ts - Y_ts)**2)\n",
    "    fold_id+=1\n",
    "\n",
    "for i in range(n_variables):\n",
    "    print(\"#Features:\", i+1, \"; CV error=\",round(np.mean(CV_err_lazy_single_model_fs[i,:]),4),\n",
    "          \"; std dev=\", round(np.std(CV_err_lazy_single_model_fs[i,:]),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a1f250",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
